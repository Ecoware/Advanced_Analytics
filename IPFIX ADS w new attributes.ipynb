{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since this is running locally I'm having some problems creating the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from datalab.context import Context\n",
    "#import datalab.storage as storage\n",
    "#import datalab.bigquery as bq\n",
    "#from google.cloud import bigquery as bq\n",
    "#import google.cloud.bigquery.job\n",
    "#from google.cloud import storage\n",
    "#from StringIO import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Query running...\n",
      "Query done.\n",
      "Processed: 550.8 kb\n",
      "\n",
      "Retrieving results...\n",
      "Got 1600 rows.\n",
      "\n",
      "Total time taken 2.32 s.\n",
      "Finished at 2017-02-06 13:32:28.\n"
     ]
    }
   ],
   "source": [
    "# Read from the ML dataset\n",
    "df = pd.read_gbq(\"select * from ML.visid_ads_final\",project_id=\"network-sec-analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Query running...\n",
      "Query done.\n",
      "Processed: 10.3 Gb\n",
      "\n",
      "Retrieving results...\n",
      "Got 1000 rows.\n",
      "\n",
      "Total time taken 4.83 s.\n",
      "Finished at 2016-10-26 20:51:08.\n"
     ]
    }
   ],
   "source": [
    "# Note that without the timestamp function a String compare function(including the UTC) with a Between will fail \n",
    "df1 = pd.read_gbq(\"select Timestamp, bytes, dst_addr, dst_port, duration_ms,protocol, flow_direction \\\n",
    "  FROM ipfix.ipfix \\\n",
    "  where Timestamp between timestamp('2016-10-14 06:13:47') and timestamp('2016-10-14 06:13:48')  \\\n",
    "    and protocol = 'tcp' limit 1000\", \n",
    "                  project_id=\"network-sec-analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(10)\n",
    "type(df1.duration_ms)\n",
    "#print(dir(bq.Dataset))\n",
    "#print(dir(ads))\n",
    "#print(dir(bq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Do a GroupBy and then convert to a dataframe.  Otherwise, the object will be a GroupByDataframe. size() is the histogram\n",
    "hist= df1.groupby([df1.Timestamp,'dst_port','protocol']).size().to_frame(name = 'count').reset_index()  \n",
    "#hist.size()  ## This will only work with a GroupByDataFrame (Drop the .size() attribute code)\n",
    "#print(dir(hist))  ## sort_values is an attribute\n",
    "hist_s=hist.sort_values(['Timestamp','count'],ascending=False)\n",
    "hist_s.loc[hist_s['count'] >= 2]  ## Or execute a SQL statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(dir(df1))\n",
    "#hist2 = df1.groupby([df1.Timestamp,'dst_port','protocol','duration_ms']).mean() \n",
    "#df1['preTestScore'].groupby([df['regiment'], df['company']]).mean().unstack()\n",
    "#df1['duration_ms'].groupby([df1['Timestamp'], df1['dst_port'], df1['protocol']]).mean()\n",
    "type(df1.duration_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Streaming Insert is 100% Complete\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.to_gbq('ipfix_samples.ipfix_sample_1', \"network-sec-analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Query running...\n",
      "Query done.\n",
      "Processed: 4.2 kb\n",
      "\n",
      "Retrieving results...\n",
      "Got 1 rows.\n",
      "\n",
      "Total time taken 1.69 s.\n",
      "Finished at 2016-10-24 15:11:08.\n"
     ]
    }
   ],
   "source": [
    "dfx = pd.read_gbq(\"select * from ipfix_samples.ipfix_sample_1 limit 1\",project_id=\"network-sec-analytics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket: gs://<PROJECT_ID>-datalab-samples\n",
      "Object: gs://<PROJECT_ID>-datalab-samples/Hello.txt\n",
      "<PROJECT_ID>\n"
     ]
    }
   ],
   "source": [
    "project = Context.default().project_id \n",
    "sample_bucket_name = project + '-datalab-samples'\n",
    "sample_bucket_path = 'gs://' + sample_bucket_name\n",
    "sample_bucket_object = sample_bucket_path + '/Hello.txt'\n",
    "\n",
    "print 'Bucket: ' + sample_bucket_path\n",
    "print 'Object: ' + sample_bucket_object\n",
    "print project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This apparently worked since I'm able to see the storage object in Cloud Storage interface\n",
    "#%%bash gsutil mb -p \"network-sec-analytics\" gs://swe-test-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP request failed: Unknown project id: 0\n"
     ]
    }
   ],
   "source": [
    "%storage list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined variable referenced in command line: $swe-test-bucket\n"
     ]
    }
   ],
   "source": [
    "%storage write --variable dfx --object $swe-test-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undefined variable referenced in command line: $swe-test-bucket\n"
     ]
    }
   ],
   "source": [
    "%%storage view --object $swe-test-bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-87-ec159e664425>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-87-ec159e664425>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sample_bucket_name = Context.default().\"network-sec-analytics\"\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "bigquery_dataset_name = 'TestDataSet'\n",
    "bigquery_table_name = 'TestTable'\n",
    "\n",
    "# Define storage bucket\n",
    "sample_bucket = storage.Bucket(sample_bucket_name)\n",
    "\n",
    "# Create storage bucket if it does not exist\n",
    "if not sample_bucket.exists():\n",
    "    sample_bucket.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AccessGrant', 'Client', 'Connection', 'Dataset', 'SCOPE', 'SchemaField', 'Table', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '__path__', '_helpers', 'client', 'connection', 'dataset', 'job', 'query', 'schema', 'table']\n"
     ]
    }
   ],
   "source": [
    "print(dir(bigquery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting query... ok.\n",
      "Query running...\n",
      "Query done.\n",
      "Cache hit.\n",
      "\n",
      "Retrieving results...\n",
      "Got 1600 rows.\n",
      "\n",
      "Total time taken 3.43 s.\n",
      "Finished at 2016-10-22 17:28:40.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
